# Neural-Network-Optimizer-
Overview
This project compares different optimization techniques for training a Convolutional Neural Network (CNN) on the CIFAR-10 dataset. The optimizers used are:

Gradient Descent (GD)
Mini-batch Stochastic Gradient Descent (SGD)
Adam
RMSprop
AdaGrad
